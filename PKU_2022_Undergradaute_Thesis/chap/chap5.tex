\chapter{开发首个$H\to WW$质量去相关的多分类标记器（基于ParticleNet）}
\label{chap5}
\fontsize{12bp}{14.4pt}

\section{质量去相关技术}
对于质量相关版本的标记器，神经网络会学习到信号样本与本底样本质量分布的差异，并且把它作为区分信号与本底的潜在判别条件，对于这样训练出来的标记器，在把本底事件误鉴别为信号事件时，会更倾向于挑选出质量分布接近信号样本的本底事件，从而对于数据中通过标记器的本底事件，在质量谱上会形成信号峰下的峰状本底，这种情况也叫做质量雕刻（mass-sculpting）。这显然对于我们从质量谱中提取信号造成了不小的困难，因此质量去相关的标记器就显得尤为重要。

从以上信息我们知道，当训练中的信号和本底样本有明显不同的质量分布时，训练出来的标记器在推理筛选本底样本时就会出现质量雕刻的现象。所以，如果训练样本的信号和本底的质量分布一致，标记器则不会学习到二者的质量信息作为区分条件，从而不会在推理筛选本底样本时出现质量雕刻现象。

要实现这点，最简单直接的办法就是在训练时对信号和本底的分布进行重新加权，但是这往往还有其他隐患，例如：
\begin{enumerate}[(1)]
    \item 如果信号样本的质量分布峰过于集中，也就是说，在信号质量窗外的事例统计量过低，这样对信号重加权时，会导致信号窗内的高峰被极大压低到和信号窗外事件相同的高度，从而造成大量真实信号没有被选中参与训练，是一种极大的浪费和欠拟合的隐患。
    \item 同时，对于远离信号窗的事例，也很难被重建出来，进一步加大了训练和推理的难度。
\end{enumerate}
综合以上两点原因，最好的做法就是尽可能使用于训练的信号样本的质量分布尽可能平滑（避免极端事例的低统计量过度压低加权），信号窗尽可能大（避免远离信号窗事例难以重建）。所以我们的做法是：设计带有一维$ m_{SD}$平分布的专用MC样本以供训练，可以通过产生并合并不同共振态质量的样本实现。（例如，产生变质量$m_X$的$X\to bb$，$X\to cc$，$X\to qq$样本以训练通用二分叉喷注标记）
\section{训练样本}
我们用到的训练样本有专门设计产生的HWW信号样本和基于官方设置产生的QCD本底样本。
\subsection{信号样本}
因为我们目标是开发一个质量去相关的标记器，所以我们的信号MC样本有以下几个特点：
\begin{enumerate}
    \item 样本基于CMS 2017 ultra-legacy，总共两千万事例。
    \item 使用变质量X的$X\to WW$衰变样本，并且设置$15 [\si{GeV}]\leq m_X\leq 250[\si{GeV}]$，$m_W=80[\si{GeV}]$。
    \item 使用JHUGen产生子实现$X\to WW\to4q/\ell \nu qq$衰变以更好模拟衰变产物的自旋关联。
    \item 经过AK8算法得到的喷注并且通过MC事实匹配打上标签。
    \item 信号训练样本中的标签分类有：\(4q,3q,e\nu qq,\mu\nu qq,\tau_e\nu qq,\tau_\mu\nu qq, \tau_h\nu qq\)
\end{enumerate}

\subsection{本底样本}
QCD本底样本有以下几个特点：
\begin{enumerate}
    \item 经过AK8算法得到的喷注并且通过MC事实匹配打上标签。
    \item 本底训练样本中的标签分类有：\(QCD(bb),QCD(cc),QCD(b),QCD(c),QCD(others)\)
\end{enumerate}

\section{标注器训练设置}
\subsection{重加权设置}
训练用的信号样本和本底样本合起来共有12个分类标签，我们要产生质量去相关的标记器，就得对用于训练的信号和本底进行质量和横向动量的二维分区间重加权。重加权的定义是：对于某个被重加权的标签分类，指定分布上的每个区间都要持有相同数量的事例数，并且来自每个分类的事例数之比要符合我们预定义的

现在我们要对信号和QCD本底样本同时在$[p_T,m]$二维分布上做重加权操作，对质量$m$的分bin区间为从20[GeV]到260[GeV]每隔10[GeV]分一个bin，对横向动量$p_T$的分bin区间为[200, 251, 316, 398, 501, 630, 793, 997, 1255, 1579, 1987, 2500]，单位为GeV。（值得注意的是，对$p_T$分bin区间宽度的选择满足指数增长，这是因为有对QCD的$p_T$分布呈指数衰减的经验分布，所以采用指数bin宽可以尽可能使得分bin后直方图高度均匀）

重加权
